import MeCab
# tokenizer = MeCab.Tagger()
tokenizer = MeCab.Tagger('-d /var/lib/mecab/dic/mecab-ipadic-neologd')

# print(tokenizer.parse('特急はくたか'))
# print(tokenizer.parse('クレアンスメアード'))

# print(tokenizer.parse('素顔'))
# print(tokenizer.parse('すがお'))
# print(tokenizer.parse('スガオ'))

print(tokenizer.parse('朝顔'))
print(tokenizer.parse('あさがお'))
print(tokenizer.parse('アサガオ'))

# print(tokenizer.parse('りんご'))
# print(tokenizer.parse('あさがお'))
# print(tokenizer.parse('いす'))
# print(tokenizer.parse('うなぎ'))
# print(tokenizer.parse('えりまきとかげ'))
# print(tokenizer.parse('おにぎり'))
# print(tokenizer.parse('ぁぃす'))
# print(tokenizer.parse('ぃヵ'))
# print(tokenizer.parse('ぅぇ'))
# print(tokenizer.parse('ぇんとらんす'))
# print(tokenizer.parse('ぉぇかき'))
# print(tokenizer.parse('かに'))
# print(tokenizer.parse('きゅうり'))
# print(tokenizer.parse('くじら'))
# print(tokenizer.parse('けいと'))
# print(tokenizer.parse('こあら'))
# print(tokenizer.parse('ヵサ'))
# print(tokenizer.parse('ヶース'))
# print(tokenizer.parse('さら'))
# print(tokenizer.parse('しらす'))
# print(tokenizer.parse('すいか'))
# print(tokenizer.parse('せっけん'))
# print(tokenizer.parse('そらまめ'))
# print(tokenizer.parse('たいこ'))
# print(tokenizer.parse('ちょこれーと'))
# print(tokenizer.parse('つみき'))
# print(tokenizer.parse('てとらぽっと'))
# print(tokenizer.parse('とまと'))
# print(tokenizer.parse('なす'))
# print(tokenizer.parse('にら'))
# print(tokenizer.parse('ぬりえ'))
# print(tokenizer.parse('ねこ'))
# print(tokenizer.parse('のり'))
# print(tokenizer.parse('はまぐり'))
# print(tokenizer.parse('ひじき'))
# print(tokenizer.parse('ふかしいも'))
# print(tokenizer.parse('へちま'))
# print(tokenizer.parse('ほうれんそう'))
# print(tokenizer.parse('まいく'))
# print(tokenizer.parse('みそしる'))
# print(tokenizer.parse('むぎわらぼうし'))
# print(tokenizer.parse('めだまやき'))
# print(tokenizer.parse('もやし'))
# print(tokenizer.parse('やくると'))
# print(tokenizer.parse('ゆうひ'))
# print(tokenizer.parse('よーぐると'))
# print(tokenizer.parse('ゃぁ'))
# print(tokenizer.parse('ゅぅ'))
# print(tokenizer.parse('ょぉ'))
# print(tokenizer.parse('らんぷ'))
# print(tokenizer.parse('りんご'))
# print(tokenizer.parse('るーれっと'))
# print(tokenizer.parse('れいとういちご'))
# print(tokenizer.parse('ろーそく'))
# print(tokenizer.parse('わさび'))
# print(tokenizer.parse('ヲタク'))
# print(tokenizer.parse('ンガイ島'))
# print(tokenizer.parse('がいこつ'))
# print(tokenizer.parse('ぎゃんぐ'))
# print(tokenizer.parse('グリンピース'))
# print(tokenizer.parse('ゲーム'))
# print(tokenizer.parse('ごもくずし'))
# print(tokenizer.parse('ざるそば'))
# print(tokenizer.parse('ジュース'))
# print(tokenizer.parse('ずわいがに'))
# print(tokenizer.parse('ぜったいぜつめい'))
# print(tokenizer.parse('ぞうりむし'))
# print(tokenizer.parse('だいだらぼっち'))
# print(tokenizer.parse('ぢわれ'))
# print(tokenizer.parse('づら'))
# print(tokenizer.parse('デパート'))
# print(tokenizer.parse('ドイツ'))
# print(tokenizer.parse('バームクーヘン'))
# print(tokenizer.parse('ビール'))
# print(tokenizer.parse('ぶた'))
# print(tokenizer.parse('べると'))
# print(tokenizer.parse('ぼーる'))
# print(tokenizer.parse('ぱぴこ'))
# print(tokenizer.parse('ぴーなっつ'))
# print(tokenizer.parse('ぷーる'))
# print(tokenizer.parse('ぺっと'))
# print(tokenizer.parse('ぽっと'))
